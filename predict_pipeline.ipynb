{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHQXK2Epvz3Os4VWKjOBoY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyammb/INNOV8TIGERS/blob/main/predict_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj0rw5I4QC3R"
      },
      "outputs": [],
      "source": [
        "!pip install rasterio==1.3.9 numpy pandas scikit-image scikit-learn xgboost lightgbm joblib tqdm shapely\n",
        "\n",
        "!pip install numpy==1.26.4\n",
        "import numpy as np\n",
        "print(\"NumPy version:\", np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, json\n",
        "import numpy as np, pandas as pd\n",
        "import rasterio\n",
        "from rasterio.warp import reproject, Resampling as WarpResampling\n",
        "from skimage.feature import local_binary_pattern, canny\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage.morphology import remove_small_objects, opening, closing, square\n",
        "from scipy.ndimage import uniform_filter\n",
        "from skimage.util import img_as_ubyte\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import xgboost as xgb, lightgbm as lgb\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "pVUjSSBKQQQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_TIF = \"/content/drive/MyDrive/HYD/S2_BOM_V2.tif\"\n",
        "MASK_TIF = \"/content/drive/MyDrive/HYD/sample_mask.tif\"\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/HYD/out_slum\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Base:\", BASE_TIF)\n",
        "print(\"Mask:\", MASK_TIF)\n",
        "print(\"Out dir:\", OUT_DIR)\n",
        "\n",
        "LBP_RADII = [1, 3]\n",
        "EDGE_WINDOWS = [3, 7]\n",
        "ROOF_WINDOWS = [3, 7]\n",
        "LACUNARITY_WINDOWS = [9, 21]\n",
        "RANK_GLCM_WINDOW = 7\n",
        "\n",
        "RF_PARAMS = {\"n_estimators\":200, \"max_depth\":12, \"class_weight\":\"balanced\", \"random_state\":42}\n",
        "XGB_PARAMS = {\"n_estimators\":300, \"max_depth\":4, \"learning_rate\":0.05, \"eval_metric\":\"logloss\"}\n",
        "LGB_PARAMS = {\"n_estimators\":500, \"num_leaves\":31, \"learning_rate\":0.05}"
      ],
      "metadata": {
        "id": "UAWm5VciQTfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_raster_meta(path):\n",
        "    with rasterio.open(path) as src:\n",
        "        names = [src.descriptions[i] or f\"Band_{i+1}\" for i in range(src.count)]\n",
        "        meta = src.meta.copy()\n",
        "    return names, meta\n",
        "\n",
        "def read_full_raster(path):\n",
        "    with rasterio.open(path) as src:\n",
        "        arr = src.read().astype(np.float32)\n",
        "        names = [src.descriptions[i] or f\"Band_{i+1}\" for i in range(src.count)]\n",
        "        meta = src.meta.copy()\n",
        "    return arr, names, meta\n",
        "\n",
        "def find_idx_by_sub(substr):\n",
        "    s=substr.upper()\n",
        "    for i,nm in enumerate(names):\n",
        "        if s in nm.upper():\n",
        "            return i\n",
        "    return None"
      ],
      "metadata": {
        "id": "qgpWYYl9QgKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading raster...\")\n",
        "arr, names, meta = read_full_raster(BASE_TIF)\n",
        "bands, H, W = arr.shape\n",
        "print(f\"Raster shape: bands={bands}, H={H}, W={W}\")\n",
        "print(\"Band names (in order):\")\n",
        "for i,nm in enumerate(names,1):\n",
        "    print(f\"{i:02d}. {nm}\")\n",
        "\n",
        "roads_tif = \"/content/drive/MyDrive/BOM_WEAKSUP/road_distance_stack.tif\"\n",
        "with rasterio.open(roads_tif) as src:\n",
        "    roads_arr = src.read(out_shape=(src.count, H, W))\n",
        "    roads_meta = src.meta\n",
        "\n",
        "roads_names = [\n",
        "    \"dist_primary\",\n",
        "    \"dist_secondary\",\n",
        "    \"dist_tertiary\",\n",
        "    \"dist_unclassified\",\n",
        "    \"dist_residential\"\n",
        "]\n",
        "\n",
        "arr = np.concatenate([arr, roads_arr], axis=0)\n",
        "names.extend(roads_names)\n",
        "bands = arr.shape[0]\n",
        "\n",
        "print(\"After adding roads bands:\", arr.shape)\n",
        "\n",
        "band_map = {}\n",
        "for key in [\n",
        "    'B2','B3','B4','B8A','B8','B11','B12',\n",
        "    'CSSI1','CSSI2','NDVI','NDBI','BSI','MNDWI','EVI',\n",
        "    'GLCM','LBP','B4_ASM','B4_CONTRAST'\n",
        "]:\n",
        "    idx = find_idx_by_sub(key)\n",
        "    if idx is not None:\n",
        "        band_map[key] = idx\n",
        "print(\"Detected band_map (0-based indices):\", band_map)\n",
        "\n",
        "mins = arr.reshape(bands, -1).min(axis=1)\n",
        "maxs = arr.reshape(bands, -1).max(axis=1)\n",
        "for i in range(bands):\n",
        "    print(f\"{i+1:02d}. {names[i]}  min={mins[i]:.6g}  max={maxs[i]:.6g}\")\n",
        "\n",
        "const_idx = [i for i in range(bands) if np.isclose(mins[i], maxs[i])]\n",
        "print(\"Constant bands (will be dropped):\", [names[i] for i in const_idx])\n",
        "keep_idx = [i for i in range(bands) if i not in const_idx]\n",
        "arr = arr[keep_idx,:,:]\n",
        "names = [names[i] for i in keep_idx]\n",
        "bands = arr.shape[0]\n",
        "print(\"Kept bands:\", len(names))\n",
        "\n",
        "ratios = []\n",
        "for i in range(bands):\n",
        "    lo, hi = arr[i].min(), arr[i].max()\n",
        "    if lo == 0:\n",
        "        r = np.inf if hi != 0 else 1.0\n",
        "    else:\n",
        "        r = abs(hi/lo)\n",
        "    ratios.append(r)\n",
        "\n",
        "crazy = [i for i,r in enumerate(ratios) if (not np.isfinite(r)) or r > 1e6 or abs(arr[i].max())>1e12]\n",
        "if crazy:\n",
        "    print(\"Bands with extreme ranges detected (will be clipped/logged/dropped):\")\n",
        "    for i in crazy:\n",
        "        print(f\" - {names[i]}  min={arr[i].min():.6g}  max={arr[i].max():.6g}\")\n",
        "\n",
        "for i in crazy:\n",
        "    band_data = arr[i].ravel()\n",
        "    p05, p995 = np.nanpercentile(band_data, [0.5, 99.5])\n",
        "    if not np.isfinite(p05) or not np.isfinite(p995) or p995==p05:\n",
        "        print(\"  dropping band (nonfinite percentiles):\", names[i])\n",
        "        arr[i,:,:] = np.nan\n",
        "    else:\n",
        "        print(f\"  clipping band {names[i]} to [{p05:.3g}, {p995:.3g}]\")\n",
        "        arr[i,:,:] = np.clip(arr[i,:,:], p05, p995)\n",
        "\n",
        "nan_bands = [i for i in range(arr.shape[0]) if np.all(np.isnan(arr[i]))]\n",
        "if nan_bands:\n",
        "    print(\"Dropping bands with all NaNs:\", [names[i] for i in nan_bands])\n",
        "    keep = [i for i in range(arr.shape[0]) if i not in nan_bands]\n",
        "    arr = arr[keep,:,:]\n",
        "    names = [names[i] for i in keep]\n",
        "\n",
        "bands = arr.shape[0]\n",
        "print(\"Final band list used:\")\n",
        "for i,nm in enumerate(names,1):\n",
        "    print(f\"{i:02d}. {nm}\")"
      ],
      "metadata": {
        "id": "4VnL9ta8eTfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import box\n",
        "\n",
        "with rasterio.open(MASK_TIF) as ms:\n",
        "    mask_arr = ms.read(1)\n",
        "    mask_meta = ms.meta.copy()\n",
        "    bounds = ms.bounds\n",
        "\n",
        "with rasterio.open(BASE_TIF) as src:\n",
        "    window = src.window(*bounds)\n",
        "    arr_crop_base = src.read(window=window)\n",
        "    transform_crop = src.window_transform(window)\n",
        "\n",
        "with rasterio.open(roads_tif) as src:\n",
        "    arr_crop_roads = src.read(window=window, out_shape=(src.count, arr_crop_base.shape[1], arr_crop_base.shape[2]))\n",
        "\n",
        "arr_crop = np.concatenate([arr_crop_base, arr_crop_roads], axis=0)\n",
        "\n",
        "\n",
        "target = np.zeros((arr_crop.shape[1], arr_crop.shape[2]), dtype=np.uint8)\n",
        "reproject(\n",
        "    source=mask_arr,\n",
        "    destination=target,\n",
        "    src_transform=mask_meta['transform'],\n",
        "    src_crs=mask_meta['crs'],\n",
        "    dst_transform=transform_crop,\n",
        "    dst_crs=src.crs,\n",
        "    resampling=WarpResampling.nearest\n",
        ")\n",
        "\n",
        "mask_aligned = (target != 0).astype(np.uint8)\n",
        "\n",
        "print(\"Mask aligned:\", mask_aligned.shape, np.unique(mask_aligned))\n",
        "print(\"Cropped satellite array:\", arr_crop.shape)\n",
        "\n",
        "\n",
        "ys, xs = np.where(~np.isnan(mask_aligned))\n",
        "print(\"Found labeled pixels:\", len(ys))\n",
        "\n",
        "\n",
        "b4_idx = None\n",
        "for k in ['B4','B8A','B8','B3']:\n",
        "    idx = find_idx_by_sub(k)\n",
        "    if idx is not None and idx in keep_idx:\n",
        "        name_found = [i for i,nm in enumerate(names) if k in nm.upper()]\n",
        "        if name_found:\n",
        "            b4_idx = name_found[0]\n",
        "            break\n",
        "if b4_idx is None:\n",
        "    intensity = np.nanmean(arr, axis=0)\n",
        "else:\n",
        "    intensity = arr[b4_idx,:,:]\n",
        "imin, imax = np.nanpercentile(intensity[~np.isnan(intensity)], [1,99])\n",
        "int_norm = (intensity - imin) / (imax - imin + 1e-9)\n",
        "int_norm = np.clip(int_norm, 0.0, 1.0)"
      ],
      "metadata": {
        "id": "ZONpYvOpeh7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature extractor - RUN EVERY TIME\n",
        "def build_feature_stack(tile_arr):\n",
        "    feats = {}\n",
        "\n",
        "    for target in ['B2','B3','B4','B8A','B8','B11','B12']:\n",
        "        idx = find_idx_by_sub(target)\n",
        "        if idx is not None:\n",
        "            try:\n",
        "                i = names.index([n for n in names if target in n.upper()][0])\n",
        "                feats[target] = tile_arr[i,:,:].astype(np.float32)\n",
        "            except Exception:\n",
        "                pass\n",
        "    for nm in ['CSSI1','CSSI2','NDVI','NDBI','BSI','MNDWI','EVI']:\n",
        "        idx = find_idx_by_sub(nm)\n",
        "        if idx is not None:\n",
        "            match = [n for n in names if nm in n.upper()]\n",
        "            if match:\n",
        "                feats[nm] = tile_arr[names.index(match[0]),:,:].astype(np.float32)\n",
        "\n",
        "    for nm in roads_names:\n",
        "        if nm in names:\n",
        "            feats[nm] = tile_arr[names.index(nm), :, :].astype(np.float32)\n",
        "\n",
        "\n",
        "    for n in names:\n",
        "        if n.upper().startswith('B4_') or n.upper().startswith('B4 '):\n",
        "            feats[n] = tile_arr[names.index(n),:,:].astype(np.float32)\n",
        "\n",
        "    for r in LBP_RADII:\n",
        "        P = 8*r\n",
        "        int_norm_safe = np.nan_to_num(int_norm, nan=0.0, posinf=1.0, neginf=0.0)\n",
        "        int_norm_safe = np.clip(int_norm_safe, 0, 1)\n",
        "        lbp = local_binary_pattern((int_norm_safe*255).astype(np.uint8), P, r, method='uniform')\n",
        "\n",
        "        feats[f'LBP_r{r}'] = ((lbp - lbp.min()) / (lbp.max() - lbp.min() + 1e-9)).astype(np.float32)\n",
        "\n",
        "\n",
        "    edges = canny(int_norm, sigma=1).astype(np.float32)\n",
        "\n",
        "    for w in EDGE_WINDOWS:\n",
        "        feats[f'EdgeFrac_{w}'] = uniform_filter(edges, size=w, mode='reflect').astype(np.float32)\n",
        "\n",
        "\n",
        "    try:\n",
        "        thr = threshold_otsu(int_norm)\n",
        "    except:\n",
        "        thr = (int_norm.mean() + int_norm.std()*0.3)\n",
        "    roofmask = (int_norm > thr).astype(np.float32)\n",
        "    for w in ROOF_WINDOWS:\n",
        "        feats[f'RoofFrac_{w}'] = uniform_filter(roofmask, size=w, mode='reflect').astype(np.float32)\n",
        "\n",
        "\n",
        "    for w in LACUNARITY_WINDOWS:\n",
        "        mean = uniform_filter(int_norm, size=w, mode='reflect')\n",
        "        mean_sq = uniform_filter(int_norm*int_norm, size=w, mode='reflect')\n",
        "        var = mean_sq - mean*mean\n",
        "        feats[f'Lacunarity_{w}'] = (var / (mean*mean + 1e-9)).astype(np.float32)\n",
        "\n",
        "    from skimage.filters import rank\n",
        "    from skimage.morphology import footprint_rectangle\n",
        "    selem = footprint_rectangle((RANK_GLCM_WINDOW,RANK_GLCM_WINDOW))\n",
        "    u8 = img_as_ubyte(int_norm)\n",
        "    feats['GLCM_entropy'] = rank.entropy(u8, selem).astype(np.float32)\n",
        "    feats['GLCM_mean'] = rank.mean(u8, selem).astype(np.float32)\n",
        "    mean = uniform_filter(int_norm.astype(np.float32), size=RANK_GLCM_WINDOW, mode='reflect')\n",
        "    mean_sq = uniform_filter((int_norm.astype(np.float32)**2), size=RANK_GLCM_WINDOW, mode='reflect')\n",
        "    feats['GLCM_std'] = np.sqrt(np.maximum(mean_sq - mean*mean, 0)).astype(np.float32)\n",
        "    feats['GLCM_min'] = rank.minimum(u8, selem).astype(np.float32)\n",
        "    feats['GLCM_max'] = rank.maximum(u8, selem).astype(np.float32)\n",
        "    return feats\n"
      ],
      "metadata": {
        "id": "6MlxIZnbendN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN EVERY TIME\n",
        "\n",
        "print(\"Building feature table for cropped region where GT exists...\")\n",
        "feats_local = build_feature_stack(arr_crop)\n",
        "feature_names = list(feats_local.keys())\n",
        "\n",
        "print(\"Built feature names:\", feature_names)\n",
        "\n",
        "ys, xs = np.where((mask_aligned==1) | (mask_aligned==0))\n",
        "rows, labels, coords = [], [], []\n",
        "for (y,x) in zip(ys,xs):\n",
        "    fv = [feats_local[f][y,x] for f in feature_names]\n",
        "    rows.append(fv)\n",
        "    labels.append(mask_aligned[y,x])\n",
        "    coords.append((x,y))\n",
        "X = np.array(rows, dtype=np.float32)\n",
        "y = np.array(labels, dtype=np.int8)\n",
        "print(\"Feature matrix shape:\", X.shape, \"Labels:\", np.bincount(y))\n",
        "\n",
        "\n",
        "var_per_feat = X.var(axis=0)\n",
        "const_feats = [feature_names[i] for i,v in enumerate(var_per_feat) if np.isclose(v,0)]\n",
        "if len(const_feats)>0:\n",
        "    print(\"Dropping constant features:\", const_feats)\n",
        "    keep_idx = [i for i in range(len(feature_names)) if feature_names[i] not in const_feats]\n",
        "    feature_names = [feature_names[i] for i in keep_idx]\n",
        "    X = X[:, keep_idx]\n",
        "\n",
        "\n",
        "scaler = RobustScaler()\n",
        "Xs = scaler.fit_transform(np.nan_to_num(X))\n",
        "\n",
        "\n",
        "pos_skew_feats = []\n",
        "for i, nm in enumerate(feature_names):\n",
        "    col = Xs[:,i]\n",
        "    if np.nanmin(col) >= 0:\n",
        "        ratio = (np.nanpercentile(col, 99.9)+1e-12) / (np.nanpercentile(col, 0.1)+1e-12)\n",
        "        if ratio > 1e3:\n",
        "            pos_skew_feats.append(nm)\n",
        "print(\"Positively skewed features (log transform):\", pos_skew_feats)\n",
        "for nm in pos_skew_feats:\n",
        "    idx = feature_names.index(nm)\n",
        "    Xs[:,idx] = np.log1p(Xs[:,idx] - np.min(Xs[:,idx]) + 1e-9)\n"
      ],
      "metadata": {
        "id": "KHBTbvnw-S48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = StratifiedKFold(n_splits=min(2, max(2,len(y)//10)), shuffle=True, random_state=42)\n",
        "def cv_eval(model):\n",
        "    aucs, f1s = [], []\n",
        "    for tr,va in cv.split(Xs,y):\n",
        "        model.fit(Xs[tr], y[tr])\n",
        "        probs = model.predict_proba(Xs[va])[:,1]\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "        aucs.append(roc_auc_score(y[va], probs))\n",
        "        f1s.append(f1_score(y[va], preds))\n",
        "    return np.mean(aucs), np.mean(f1s)\n",
        "\n",
        "rf  = RandomForestClassifier(**RF_PARAMS)\n",
        "xg  = xgb.XGBClassifier(**XGB_PARAMS, use_label_encoder=False)\n",
        "lg  = lgb.LGBMClassifier(**LGB_PARAMS)\n",
        "log = LogisticRegression(class_weight='balanced', solver='liblinear')\n",
        "\n",
        "models = {'RandomForest':rf, 'XGBoost':xg, 'LightGBM':lg, 'Logistic':log}\n",
        "results, trained, importances = {}, {}, {}\n",
        "\n",
        "for name,m in models.items():\n",
        "    try:\n",
        "        auc, f1 = cv_eval(m)\n",
        "        results[name] = {'AUC':auc, 'F1':f1}\n",
        "        print(f\"{name} CV -> AUC: {auc:.4f}, F1: {f1:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{name} CV failed:\", e)\n",
        "\n",
        "for name,m in models.items():\n",
        "    print(\"Training full model on cropped GT region:\", name)\n",
        "    m.fit(Xs, y)\n",
        "    trained[name] = m\n",
        "    if hasattr(m, 'feature_importances_'):\n",
        "        imps = m.feature_importances_\n",
        "        importances[name] = pd.Series(imps, index=feature_names).sort_values(ascending=False)\n",
        "    elif hasattr(m, 'coef_'):\n",
        "        importances[name] = pd.Series(np.abs(m.coef_).ravel(), index=feature_names).sort_values(ascending=False)\n",
        "    else:\n",
        "        importances[name] = pd.Series(0, index=feature_names)\n",
        "    print(\" Top features:\", importances[name].head(8).to_dict())\n",
        "\n",
        "imp_df = pd.concat(importances).reset_index()\n",
        "imp_df.columns = ['model','feature','importance']\n",
        "agg = imp_df.groupby('feature').importance.mean().sort_values(ascending=False)\n",
        "print(\"Aggregated importance (top 20):\")\n",
        "print(agg.head(20))\n",
        "\n",
        "useless = agg[agg < 1e-3].index.tolist()\n",
        "print(\"Useless features candidates (mean importance < 1e-3):\", useless)\n",
        "\n",
        "import joblib\n",
        "\n",
        "MODELS_DIR = os.path.join(OUT_DIR, \"saved_models\")\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "for name, model in trained.items():\n",
        "    model_path = os.path.join(MODELS_DIR, f\"{name}.joblib\")\n",
        "    joblib.dump(model, model_path)\n",
        "    print(f\"Saved model: {name} -> {model_path}\")\n",
        "\n",
        "\n",
        "summary = {\n",
        "    \"kept_band_names\": names,\n",
        "    \"feature_names\": feature_names,\n",
        "    \"cv_results\": results,\n",
        "    \"agg_importance\": agg.head(50).to_dict(),\n",
        "    \"useless_candidates\": useless\n",
        "}\n",
        "with open(os.path.join(OUT_DIR, \"pipeline_summary.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"Saved pipeline summary to\", os.path.join(OUT_DIR, \"pipeline_summary.json\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "xO_dDVxr-YGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from rasterio.warp import reproject, Resampling\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "\n",
        "tile_size = 1024\n",
        "stride = 1024\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/HYD/out_slum/\"\n",
        "\n",
        "full_raster_path = \"/content/drive/MyDrive/HYD/S2_BOM_V2.tif\"\n",
        "full_gt_path = \"/content/drive/MyDrive/HYD/reprojected_mask_aligned.tif\"\n",
        "roads_tif = \"/content/drive/MyDrive/HYD/road_distance_stack.tif\"\n",
        "MODELS_DIR = \"/content/drive/MyDrive/HYD/out_slum/saved_models/\"\n",
        "\n",
        "\n",
        "loaded_models = {}\n",
        "for fname in os.listdir(MODELS_DIR):\n",
        "    if fname.endswith(\".joblib\"):\n",
        "        model_name = os.path.splitext(fname)[0]\n",
        "        model_path = os.path.join(MODELS_DIR, fname)\n",
        "        loaded_models[model_name] = joblib.load(model_path)\n",
        "        print(f\"Loaded model: {model_name} from {model_path}\")\n",
        "\n",
        "\n",
        "def process_tile(arr_tile, feature_names, scaler, trained_model):\n",
        "    feats_tile = build_feature_stack(arr_tile)\n",
        "    ys, xs = np.where(np.ones((arr_tile.shape[1], arr_tile.shape[2]), dtype=bool))\n",
        "    rows_tile = []\n",
        "    for y, x in zip(ys, xs):\n",
        "        fv = [feats_tile[f][y, x] for f in feature_names]\n",
        "        rows_tile.append(fv)\n",
        "    X_tile = np.array(rows_tile, dtype=np.float32)\n",
        "    X_tile_scaled = scaler.transform(np.nan_to_num(X_tile))\n",
        "    y_pred = trained_model.predict(X_tile_scaled)\n",
        "    pred_tile = np.zeros((arr_tile.shape[1], arr_tile.shape[2]), dtype=np.uint8)\n",
        "    pred_tile[ys, xs] = y_pred\n",
        "    return pred_tile\n",
        "\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "with rasterio.open(full_raster_path) as src_full, rasterio.open(roads_tif) as src_roads:\n",
        "    H, W = src_full.height, src_full.width\n",
        "    profile = src_full.profile\n",
        "    profile.update(dtype=rasterio.uint8, count=1, compress=\"lzw\")\n",
        "\n",
        "    for name, model in loaded_models.items():\n",
        "        print(f\"Predicting full raster with {name}...\")\n",
        "\n",
        "        out_tif = os.path.join(OUT_DIR, f\"pred_{name}.tif\")\n",
        "        with rasterio.open(out_tif, \"w\", **profile) as dst:\n",
        "\n",
        "            for i in range(0, H, stride):\n",
        "                for j in range(0, W, stride):\n",
        "                    h = min(tile_size, H - i)\n",
        "                    w = min(tile_size, W - j)\n",
        "\n",
        "                    arr_tile_base = src_full.read(window=Window(j, i, w, h),\n",
        "                                                  out_shape=(src_full.count, h, w))\n",
        "                    arr_tile_roads = src_roads.read(window=Window(j, i, w, h),\n",
        "                                                    out_shape=(src_roads.count, h, w))\n",
        "                    arr_tile = np.concatenate([arr_tile_base, arr_tile_roads], axis=0)\n",
        "\n",
        "                    pred_tile = process_tile(arr_tile, feature_names, scaler, model)\n",
        "\n",
        "                    dst.write(pred_tile, 1, window=Window(j, i, w, h))\n",
        "\n",
        "        print(f\"Saved predicted TIF for {name} -> {out_tif}\")\n",
        "        with rasterio.open(full_gt_path) as src_gt:\n",
        "            gt_data = src_gt.read(1)\n",
        "            gt_meta = src_gt.meta.copy()\n",
        "\n",
        "        gt_aligned = np.zeros((H, W), dtype=np.uint8)\n",
        "        reproject(\n",
        "            source=gt_data,\n",
        "            destination=gt_aligned,\n",
        "            src_transform=gt_meta[\"transform\"],\n",
        "            src_crs=gt_meta[\"crs\"],\n",
        "            dst_transform=profile[\"transform\"],\n",
        "            dst_crs=profile[\"crs\"],\n",
        "            resampling=Resampling.nearest\n",
        "        )\n",
        "\n",
        "        with rasterio.open(out_tif) as src_pred:\n",
        "            pred_full = src_pred.read(1)\n",
        "\n",
        "        mask = (gt_aligned >= 0)\n",
        "        y_true = gt_aligned[mask].ravel()\n",
        "        y_pred_masked = pred_full[mask].ravel()\n",
        "\n",
        "        precision = precision_score(y_true, y_pred_masked, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred_masked, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred_masked, zero_division=0)\n",
        "        print(f\"{name} -> Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "        evaluation_results[name] = {\n",
        "            \"precision\": float(precision),\n",
        "            \"recall\": float(recall),\n",
        "            \"f1\": float(f1)\n",
        "        }\n",
        "\n",
        "eval_path = os.path.join(OUT_DIR, \"evaluation_full_gt.json\")\n",
        "with open(eval_path, \"w\") as f:\n",
        "    json.dump(evaluation_results, f, indent=2)\n",
        "print(\"Saved full GT evaluation results to\", eval_path)\n"
      ],
      "metadata": {
        "id": "JpXI77H4-gE8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}